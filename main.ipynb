{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recupera todos los enlaces de artículos de una página específica (por mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    \n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\" : ua.random}\n",
    "        data = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(data.text, \"html5lib\")\n",
    "        \n",
    "        all_links = []\n",
    "        \n",
    "        contents = soup.find_all(\"div\", class_=\"single-blog-content\")\n",
    "        \n",
    "        for content in contents:\n",
    "            links = content.find_all(\"a\", target = \"_blank\")\n",
    "        \n",
    "            for link in links:\n",
    "                all_links.append(link.get(\"href\"))\n",
    "        \n",
    "        return all_links\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error en la URL {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construye la URL de la página correspondiente al mes y llama a get_links() para obtener los enlaces de artículos publicados en ese mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_links_in_month(month):\n",
    "    url = \"https://www.lajornadamaya.mx/k'iintsil/{}-2021\".format(month)\n",
    "    return get_links(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extrae la información de un artículo a partir de su enlace, extrayendo Fecha - Titulo - Contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(link):\n",
    "    \n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        headers = {\"User-Agent\" : ua.random}\n",
    "        data = requests.get(link, headers=headers)\n",
    "        soup = BeautifulSoup(data.text, \"html5lib\")\n",
    "        \n",
    "        # Busqueda del titulo\n",
    "        try:\n",
    "            title = soup.find(\"h1\").get_text(strip=True) \n",
    "        except AttributeError:\n",
    "            print(f\"Título no encontrado: {link}\")\n",
    "            title = \"Sin Titulo\"\n",
    "        # Opcional: Busqueda de la ubicacion\n",
    "        # Busqueda de la fecha\n",
    "        try:\n",
    "            info = soup.find(\"div\", class_=\"post-meta\")\n",
    "            if not info:\n",
    "                raise ValueError(\"Informacion no encontrada\")\n",
    "            \n",
    "            date_pattern = r\"\\d{2}/\\d{2}/\\d{4}\"\n",
    "            date_match = re.search(date_pattern, info.get_text())\n",
    "            if not date_match:\n",
    "                raise ValueError(\"Fecha no encontrada\")\n",
    "             \n",
    "            date = date_match.group().strip() \n",
    "            \n",
    "        except (AttributeError, ValueError) as e:\n",
    "            print(f\"Error al extraer la fecha: {link}\")\n",
    "            date = \"Fecha desconocida\"\n",
    "        \n",
    "        # Busqueda del contenido\n",
    "        try:\n",
    "            article = info.find_next(\"p\").find_next(\"p\")\n",
    "            text = \"\"\n",
    "            if not article:\n",
    "                raise ValueError(\"Articulo no encontrado\")\n",
    "            \n",
    "            paragraphs = article.find_all_next(\"p\")\n",
    "            for paragraph in paragraphs :\n",
    "                if not paragraph.find(\"strong\"):\n",
    "                    text += paragraph.get_text() + \"\\n\"\n",
    "                \n",
    "        except (AttributeError, ValueError) as e:\n",
    "            print(f\"Error al extraer el articulo: {link}\")\n",
    "            text = \"Contenido no disponible\"\n",
    "            \n",
    "        # Opcional: Busqueda del autor\n",
    "        # Opcional: Busqueda del traductor\n",
    "        \n",
    "        return {\n",
    "            \"Fecha\": date,\n",
    "            #\"Author\": author,\n",
    "            #\"Traductor\": traductor,\n",
    "            #\"Ubicacion\": location,\n",
    "            \"Titulo\": title,\n",
    "            \"Contenido\": text.strip()\n",
    "        }\n",
    "         \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error en la URL {link}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesa el scraping para cada mes del año\n",
    " - Recuperando los enlaces y los detalles de cada artículo\n",
    " - Guardándolos en un archivo .csv llamado datos.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping():\n",
    "    months = [\"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\", \"julio\", \"agosto\", \"septiembre\", \"octubre\", \"noviembre\", \"diciembre\"]\n",
    "    df = pd.DataFrame()\n",
    "    dfs = []\n",
    "    for month in months:\n",
    "        all_links = find_all_links_in_month(month)\n",
    "        \n",
    "        data_month = []\n",
    "    \n",
    "        print(f\"Mes:{month}, Total: {len(all_links)} \\n\")\n",
    "        \n",
    "        for link in all_links:\n",
    "            if link: \n",
    "                info = get_info(link)\n",
    "                if info:\n",
    "                    data_month.append(info)\n",
    "            \n",
    "        df_month = pd.DataFrame(data_month)\n",
    "        dfs.append(df_month)\n",
    "        \n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    df.to_csv(\"datos.csv\", index=False, encoding='utf-8')\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mes:enero, Total: 55 \n",
      "\n",
      "Mes:febrero, Total: 51 \n",
      "\n",
      "Mes:marzo, Total: 61 \n",
      "\n",
      "Mes:abril, Total: 54 \n",
      "\n",
      "Mes:mayo, Total: 59 \n",
      "\n",
      "Mes:junio, Total: 64 \n",
      "\n",
      "Mes:julio, Total: 67 \n",
      "\n",
      "Mes:agosto, Total: 66 \n",
      "\n",
      "Mes:septiembre, Total: 56 \n",
      "\n",
      "Mes:octubre, Total: 68 \n",
      "\n",
      "Mes:noviembre, Total: 66 \n",
      "\n",
      "Mes:diciembre, Total: 52 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scraping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
